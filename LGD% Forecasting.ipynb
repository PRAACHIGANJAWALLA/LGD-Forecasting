{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c599c0f2",
   "metadata": {},
   "source": [
    "Adding the imports for the Project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32f1da61",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import decimal\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import subplots\n",
    "from matplotlib.ticker import StrMethodFormatter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed7abe1f",
   "metadata": {},
   "source": [
    "Reading the Raw Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae300a11",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Read the Excel file\n",
    "df = pd.read_excel('NewDataset.xlsx')\n",
    "\n",
    "# Display the DataFrame\n",
    "df.iloc[:, :,]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6cd7d64",
   "metadata": {},
   "source": [
    "Getting to know the data types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15787fc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_types = df.dtypes\n",
    "print(data_types)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aab8cf8",
   "metadata": {},
   "source": [
    "Changing the data types as per the data dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e8ca1fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['LIMIT'] = df['LIMIT'].astype(float).round(2)\n",
    "# Replace NaN with a specific value (e.g., 0)\n",
    "df['LOAN_TYPE'] = df['LOAN_TYPE'].fillna(0).astype(int)\n",
    "df['PRODUCT_NAME'] = df['PRODUCT_NAME'].astype(str).str.ljust(250)\n",
    "df['TIMES_DELQ_30_DAYS'] = df['TIMES_DELQ_30_DAYS'].fillna(0).astype(int)\n",
    "df['TIMES_DELQ_60_DAYS'] = df['TIMES_DELQ_60_DAYS'].fillna(0).astype(int)\n",
    "df['TIMES_DELQ_90_DAYS'] = df['TIMES_DELQ_90_DAYS'].fillna(0).astype(int)\n",
    "df['DAYSDEL'] = df['DAYSDEL'].fillna(0).astype(int)\n",
    "df['CREDIT_SCORE'] = df['CREDIT_SCORE'].fillna(0).astype(int)\n",
    "df['CALC_LOANSTATUS'] = df['CALC_LOANSTATUS'].astype(str).str.ljust(40)\n",
    "df['MONTHS_OLD'] = df['MONTHS_OLD'].fillna(0).fillna(0).astype(int)\n",
    "df['SEASONED_OVER_2YR'] = df['SEASONED_OVER_2YR'].fillna(0).astype(int)\n",
    "df['BANKPRUPTCY_FLAG'] = df['BANKPRUPTCY_FLAG'].fillna(0).astype(int)\n",
    "df['STATE'] = df['STATE'].astype(str).str.ljust(144)\n",
    "df['LOYALTY_LEVEL_ID'] = df['LOYALTY_LEVEL_ID'].fillna(0).astype(int)\n",
    "df['LOYALTY_LEVEL_DESC'] = df['LOYALTY_LEVEL_DESC'].astype(str).str.ljust(12)\n",
    "df['MEMBER_TYPE'] = df['MEMBER_TYPE'].astype(str).str.ljust(8)\n",
    "df['BANKRUPTCY_INDEX'] = df['BANKRUPTCY_INDEX'].fillna(0).astype(int)\n",
    "df['HAS_DIRECT_DEP'] = df['HAS_DIRECT_DEP'].fillna(0).astype(int)\n",
    "df['CALC_CURRENT_MONTH_END_SHARE_BALANCE'] = df['CALC_CURRENT_MONTH_END_SHARE_BALANCE'].fillna(0).astype(float)\n",
    "df['Calc_DirectIndirect'] = df['Calc_DirectIndirect'].astype(str).str.ljust(8)\n",
    "df['NEG_SHARE_BALANCE'] = df['NEG_SHARE_BALANCE'].fillna(0).astype(int)\n",
    "df['LOYALTY_DUMMY_RELATIONSHIP'] = df['LOYALTY_DUMMY_RELATIONSHIP'].fillna(0).astype(int)\n",
    "df['LOYALTY_DUMMY_PLUS'] = df['LOYALTY_DUMMY_PLUS'].fillna(0).astype(int)\n",
    "df['LOYALTY_DUMMY_MEMBER'] = df['LOYALTY_DUMMY_MEMBER'].fillna(0).astype(int)\n",
    "df['ORIG_LOAN_COUNT_LESSTHAN2_DUMMY'] = df['ORIG_LOAN_COUNT_LESSTHAN2_DUMMY'].fillna(0).astype(int)\n",
    "df['INT_ACCRUED'] = df['INT_ACCRUED'].fillna(0).astype(float)\n",
    "df['CALC_PRIOR_MONTH_END_SHARE_BALANCE'] = df['CALC_PRIOR_MONTH_END_SHARE_BALANCE'].fillna(0).astype(float)\n",
    "df['MEMBER_TYPE'] = df['MEMBER_TYPE'].astype(str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f4972b3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Making sure LGD% is between 0 & 1 and replacing anything else with Nan\n",
    "\n",
    "df.loc[(df['LGD%'] > 1) | (df['LGD%'] < 0), 'LGD%'] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b81ba1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d3a7d82",
   "metadata": {},
   "source": [
    "Deleting the Unneccessary columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "501dc828",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "\n",
    "columns_to_delete = ['LoanDescription', 'LoanType', 'DAYSDEL', 'LOYALTY_LEVEL_DESC', 'Calc_DirectIndirect','LOYALTY_DUMMY_RELATIONSHIP', 'LOYALTY_DUMMY_PLUS', 'ORIG_LOAN_COUNT_LESSTHAN2_DUMMY', 'CLOSE_DATE', 'FINAL_PMT_DATE']\n",
    "\n",
    "df.drop(columns=columns_to_delete, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0726547",
   "metadata": {},
   "source": [
    "Removing the missing rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9db7307e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(axis=0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30afa6e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1765e707",
   "metadata": {},
   "outputs": [],
   "source": [
    "column_names = df.columns\n",
    "print(column_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e982366",
   "metadata": {},
   "source": [
    "For Visualization purpose only:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24d2cbee",
   "metadata": {},
   "outputs": [],
   "source": [
    "fp = r\"New_trial.xlsx\"\n",
    "df.to_excel(fp, sheet_name='Edit 2')\n",
    "\n",
    "\n",
    "print(\"Modified data saved to\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d97670e",
   "metadata": {},
   "source": [
    "Replace the categories with 0 & 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6dd95ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['BANKPRUPTCY_FLAG'] = df['BANKPRUPTCY_FLAG'].replace({'FALSE': 0, 'TRUE': 1})\n",
    "col = df['MEMBER_TYPE'].dtype\n",
    "print(col)\n",
    "df['PRODUCT_NAME'] = df['PRODUCT_NAME'].str.strip()\n",
    "df['MEMBER_TYPE'] = df['MEMBER_TYPE'].replace({'Personal' : 0 , 'Business' : 1})\n",
    "\n",
    "\n",
    "df['PRODUCT_NAME'] = df['PRODUCT_NAME'].replace({'DCU Visa': 0, 'DCU Rewards Visa': 1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c3bfaa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_columns = df[['BANKPRUPTCY_FLAG', 'PRODUCT_NAME' , 'MEMBER_TYPE']]\n",
    "print(selected_columns[['PRODUCT_NAME' , 'MEMBER_TYPE']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2680c967",
   "metadata": {},
   "source": [
    "Created a New column named new_england which consists data only of the New England states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b346c08",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['STATE'] = df['STATE'].str[0:2]\n",
    "df.head()\n",
    "new_england_states = ['MA','NH','ME','CT','VT','RI']\n",
    "df['new_england'] = df['STATE'].apply(lambda x:0 if x in new_england_states else 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cc1a9a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_of_0 = (df['new_england'] == 0).sum()\n",
    "count_of_1 = (df['new_england'] == 1).sum()\n",
    "print(count_of_0)\n",
    "print(count_of_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7db4c56d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping State Column as we have already trnsformed it into New-England states.\n",
    "\n",
    "df.drop(columns=['STATE'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a2a5ee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write DataFrame to Excel file with sheet name\n",
    "fp = r\"New.xlsx\"\n",
    "df.to_excel(fp, sheet_name='Edit 1')\n",
    "\n",
    "\n",
    "print(\"Modified data saved to\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfe945b7",
   "metadata": {},
   "source": [
    "Histograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4e7595c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.hist(figsize=(15, 40), layout=(11,3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57fc6a83",
   "metadata": {},
   "source": [
    "Importing various packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "396d7e1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.stats.outliers_influence \\\n",
    "import variance_inflation_factor as VIF\n",
    "from statsmodels.stats.anova import anova_lm\n",
    "import ISLP\n",
    "from ISLP import load_data\n",
    "from ISLP.models import (ModelSpec as MS,\n",
    "summarize ,\n",
    "poly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d770f5b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "616a4839",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_types = df.dtypes\n",
    "print(data_types)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c084324",
   "metadata": {},
   "source": [
    "Creating new columns to show differences between the 2 dates in the data provided (just to find a corelation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4e3b46a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new columns for the differences from FIRST_CO_MONTH_END\n",
    "df['Diff_FIRST_CO_MONTH_END'] = df['FIRST_CO_Month_End_Date'] - df['MONTH_END_DATE']\n",
    "df['Diff_FIRST_CO_CREDIT_SCORE'] = df['FIRST_CO_Month_End_Date'] - df['CREDIT_SCORE_DATE']\n",
    "df['Diff_FIRST_CO_BANKRUPTCY'] = df['FIRST_CO_Month_End_Date'] - df['BANKRUPTCY_INDEX_DATE']\n",
    "# Create new columns for the differences from MONTH_END_DATE                   \n",
    "df['Diff_MONTH_END_CREDIT_SCORE'] = df['MONTH_END_DATE'] - df['CREDIT_SCORE_DATE']\n",
    "df['Diff_MONTH_END_BANKRUPTCY'] = df['MONTH_END_DATE'] - df['BANKRUPTCY_INDEX_DATE']\n",
    "# Create new columns for the differences from OPEN_DATE\n",
    "df['Diff_OPEN_DATE_FIRST_CO'] = df['FIRST_CO_Month_End_Date'] - df['OPEN_DATE']\n",
    "df['Diff_OPEN_DATE_MONTH_END'] = df['MONTH_END_DATE'] - df['OPEN_DATE']\n",
    "df['Diff_OPEN_DATE_CREDIT_SCORE'] = df['CREDIT_SCORE_DATE'] - df['OPEN_DATE']\n",
    "df['Diff_OPEN_DATE_BANKRUPTCY'] = df['BANKRUPTCY_INDEX_DATE'] - df['OPEN_DATE']\n",
    "\n",
    "# Convert them into days\n",
    "df['Diff_FIRST_CO_MONTH_END'] = df['Diff_FIRST_CO_MONTH_END'].dt.days\n",
    "df['Diff_FIRST_CO_CREDIT_SCORE'] = df['Diff_FIRST_CO_CREDIT_SCORE'].dt.days\n",
    "df['Diff_FIRST_CO_BANKRUPTCY'] = df['Diff_FIRST_CO_BANKRUPTCY'].dt.days\n",
    "df['Diff_MONTH_END_CREDIT_SCORE'] = df['Diff_MONTH_END_CREDIT_SCORE'].dt.days\n",
    "df['Diff_MONTH_END_BANKRUPTCY'] = df['Diff_MONTH_END_BANKRUPTCY'].dt.days\n",
    "df['Diff_OPEN_DATE_FIRST_CO'] = df['Diff_OPEN_DATE_FIRST_CO'].dt.days\n",
    "df['Diff_OPEN_DATE_MONTH_END'] = df['Diff_OPEN_DATE_MONTH_END'].dt.days\n",
    "df['Diff_OPEN_DATE_CREDIT_SCORE'] = df['Diff_OPEN_DATE_CREDIT_SCORE'].dt.days\n",
    "df['Diff_OPEN_DATE_BANKRUPTCY'] = df['Diff_OPEN_DATE_BANKRUPTCY'].dt.days\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bf967ce",
   "metadata": {},
   "source": [
    "Dropping Date columns to avoid date-time format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a60ff03e",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_drop = ['FIRST_CO_Month_End_Date', 'CREDIT_SCORE_DATE', 'BANKRUPTCY_INDEX_DATE', 'OPEN_DATE']\n",
    "\n",
    "df.drop(columns=columns_to_drop, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18df8f0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop CALC_LOANSTATUS \n",
    "\n",
    "df.drop(columns='CALC_LOANSTATUS', inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74acb1ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df[['Diff_FIRST_CO_MONTH_END', 'Diff_FIRST_CO_CREDIT_SCORE', 'Diff_FIRST_CO_BANKRUPTCY', 'Diff_MONTH_END_CREDIT_SCORE', 'Diff_MONTH_END_BANKRUPTCY','Diff_OPEN_DATE_MONTH_END','Diff_OPEN_DATE_CREDIT_SCORE', 'Diff_OPEN_DATE_BANKRUPTCY' ]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbf6f53a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df[['Diff_OPEN_DATE_MONTH_END','Diff_OPEN_DATE_FIRST_CO', 'Diff_OPEN_DATE_CREDIT_SCORE', 'Diff_OPEN_DATE_BANKRUPTCY' ]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35614ab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Created df_trial to show the stepwise VIF in this\n",
    "df_trial = df.copy()\n",
    "\n",
    "# Drop 'MONTH_END_DATE' column from df3\n",
    "df_trial = df_trial.drop('MONTH_END_DATE', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45dd6f7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "terms = df_trial.columns.drop('Recovery')\n",
    "print(terms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7fbcb1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "column_names = df_trial.columns\n",
    "print(column_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "775cf388",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_trial.dtypes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c117c3c8",
   "metadata": {},
   "source": [
    "Split the data into train and test sets on the basis of years in MONTH_END_DATE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c74e4eaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "train_set = df.loc[(df['MONTH_END_DATE'] >= '2013-01-01') & (df['MONTH_END_DATE'] <= '2017-12-31'), :]\n",
    "\n",
    "test_set_1 = df.loc[(df['MONTH_END_DATE'] >= '2018-01-01') & (df['MONTH_END_DATE'] <= '2020-03-31'), :]\n",
    "test_set_2 = df.loc[(df['MONTH_END_DATE'] >= '2020-04-01') & (df['MONTH_END_DATE'] <= '2020-12-31'), :]\n",
    "test_set_3 = df.loc[(df['MONTH_END_DATE'] >= '2021-01-01') & (df['MONTH_END_DATE'] <= '2021-12-31'), :]\n",
    "test_set_4 = df.loc[(df['MONTH_END_DATE'] >= '2022-01-01') & (df['MONTH_END_DATE'] <= '2022-12-31'), :]\n",
    "\n",
    "\n",
    "print(\"Train Set Shape:\", train_set.shape)\n",
    "print(\"Test Set 1 Shape:\", test_set_1.shape)\n",
    "print(\"Test Set 2 Shape:\", test_set_2.shape)\n",
    "print(\"Test Set 3 Shape:\", test_set_3.shape)\n",
    "print(\"Test Set 4 Shape:\", test_set_4.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c290b5c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Columns of Train Set:\", train_set.columns)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "722d4fb6",
   "metadata": {},
   "source": [
    "Drop MED to remove the date-time format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f792e624",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop 'MONTH_END_DATE' column from train_set\n",
    "train_set = train_set.drop('MONTH_END_DATE', axis=1)\n",
    "\n",
    "# Drop 'MONTH_END_DATE' column from test_set_1\n",
    "test_set_1 = test_set_1.drop('MONTH_END_DATE', axis=1)\n",
    "\n",
    "# Drop 'MONTH_END_DATE' column from test_set_2\n",
    "test_set_2 = test_set_2.drop('MONTH_END_DATE', axis=1)\n",
    "\n",
    "# Drop 'MONTH_END_DATE' column from test_set_3\n",
    "test_set_3 = test_set_3.drop('MONTH_END_DATE', axis=1)\n",
    "\n",
    "# Drop 'MONTH_END_DATE' column from test_set_4\n",
    "test_set_4 = test_set_4.drop('MONTH_END_DATE', axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf4eb1d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Columns of Train Set:\", train_set.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fea77d8a",
   "metadata": {},
   "source": [
    "VIF Process Starts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2970d33f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = MS(terms).fit_transform(df_trial)\n",
    "y = df_trial['Recovery']\n",
    "model = sm.OLS(y, X)\n",
    "results = model.fit()\n",
    "summarize(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e00c6e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "vals = [VIF(X, i)\n",
    "for i in range(1, X.shape[1])]\n",
    "vif = pd.DataFrame({'vif':vals},\n",
    "index=X.columns [1:])\n",
    "vif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f25b5c8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78d35dfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vif(dfx):\n",
    "    vals = [VIF(dfx, i) for i in range(1, dfx.shape[1])]\n",
    "    vif_var = pd.DataFrame({'vif': vals}, index=dfx.columns[1:])\n",
    "    return vif_var"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2a1e980",
   "metadata": {},
   "source": [
    "\"Recovery\" is the dependent variable, and we are excluding it out of our dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "655de12c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "independent_variables = df_trial.drop(columns=['Recovery'])\n",
    "\n",
    "result = vif(independent_variables)\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9087331",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try : Removing MEMBER_TYPE since it has Nan\n",
    "\n",
    "independent_variables1 = independent_variables.drop(columns=['MEMBER_TYPE'])\n",
    "result = vif(independent_variables1)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b6e41c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try 1: Removing Diff_FIRST_CO_MONTH_END\n",
    "\n",
    "\n",
    "independent_variables2 = independent_variables1.drop(columns=['Diff_FIRST_CO_MONTH_END'])\n",
    "\n",
    "result = vif(independent_variables2)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90bfa221",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try 2: Removing Diff_FIRST_CO_CREDIT_SCORE\n",
    "\n",
    "independent_variables3 = independent_variables2.drop(columns=['Diff_FIRST_CO_CREDIT_SCORE'])\n",
    "\n",
    "result = vif(independent_variables3)\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7611b186",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try 3: Removing Diff_FIRST_CO_BANKRUPTCY\n",
    "\n",
    "\n",
    "independent_variables4 = independent_variables3.drop(columns=['Diff_FIRST_CO_BANKRUPTCY'])\n",
    "\n",
    "result = vif(independent_variables4)\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ea3e4cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Try 4: Removing Diff_MONTH_END_CREDIT_SCORE\n",
    "\n",
    "\n",
    "independent_variables5 = independent_variables4.drop(columns=['Diff_MONTH_END_CREDIT_SCORE'])\n",
    "\n",
    "result = vif(independent_variables5)\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62b4c685",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try 5: Removing Diff_MONTH_END_BANKRUPTCY\n",
    "\n",
    "independent_variables6 = independent_variables5.drop(columns=['Diff_MONTH_END_BANKRUPTCY'])\n",
    "\n",
    "result = vif(independent_variables6)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78c12bc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Try 6: Removing Diff_OPEN_DATE_MONTH_END\n",
    "\n",
    "independent_variables7 = independent_variables6.drop(columns=['Diff_OPEN_DATE_MONTH_END'])\n",
    "\n",
    "result = vif(independent_variables7)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60137469",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Try 7: Removing Highest VIF LOAN_TYPE\n",
    "\n",
    "independent_variables8 = independent_variables7.drop(columns=['LOAN_TYPE'])\n",
    "\n",
    "result = vif(independent_variables8)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a128c6ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Try 8: Removing Highest VIF Diff_OPEN_DATE_BANKRUPTCY\n",
    "independent_variables9 = independent_variables8.drop(columns=['Diff_OPEN_DATE_BANKRUPTCY'])\n",
    "\n",
    "result = vif(independent_variables9)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e94bad19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try 9: Removing Highest VIF MONTHS_OLD\n",
    "\n",
    "independent_variables10 = independent_variables9.drop(columns=['MONTHS_OLD'])\n",
    "\n",
    "result = vif(independent_variables10)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab78de8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try 10: Removing Highest VIF LOAN_NBR\n",
    "independent_variables11 = independent_variables10.drop(columns=['LOAN_NBR'])\n",
    "\n",
    "result = vif(independent_variables11)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51b1d49a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try 11: Removing Highest VIF Diff_OPEN_DATE_FIRST_CO\n",
    "independent_variables12 = independent_variables11.drop(columns=['Diff_OPEN_DATE_FIRST_CO'])\n",
    "\n",
    "result = vif(independent_variables12)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ca8fe72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try 12: Removing Highest VIF LGD%\n",
    "independent_variables13 = independent_variables12.drop(columns=['LGD%'])\n",
    "\n",
    "result = vif(independent_variables13)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a97cb5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try 13: Removing Highest VIF Charge-Off\n",
    "independent_variables14 = independent_variables13.drop(columns=['Charge-Off'])\n",
    "\n",
    "result = vif(independent_variables14)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bc42c91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try 14: Removing Highest VIF NCO\n",
    "independent_variables15 = independent_variables14.drop(columns=['NCO'])\n",
    "\n",
    "result = vif(independent_variables15)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71c9d658",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try 15: Removing Highest VIF TIMES_DELQ_60_DAYS\n",
    "independent_variables16 = independent_variables15.drop(columns=['TIMES_DELQ_60_DAYS'])\n",
    "\n",
    "result = vif(independent_variables16)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51987c4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try 16: Removing Highest VIF LOYALTY_LEVEL_ID\n",
    "independent_variables17 = independent_variables16.drop(columns=['LOYALTY_LEVEL_ID'])\n",
    "\n",
    "result = vif(independent_variables17)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aaf89f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try 17: Removing Highest VIF BANKRUPTCY_INDEX\n",
    "independent_variables18 = independent_variables17.drop(columns=['BANKRUPTCY_INDEX'])\n",
    "\n",
    "result = vif(independent_variables18)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"yellow\">MODEL 1 : LOGISTIC REGRESSION</font>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15a6c5dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47e84f49",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "from patsy import dmatrix\n",
    "from sklearn.utils import resample\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "\n",
    "# Target Variable\n",
    "target_column = 'Recovery'\n",
    "\n",
    "# Columns\n",
    "allvars = df[['BALANCE', 'LIMIT', 'PRODUCT_NAME', 'TIMES_DELQ_30_DAYS', 'TIMES_DELQ_90_DAYS', 'CREDIT_SCORE', 'SEASONED_OVER_2YR', 'BANKPRUPTCY_FLAG', 'HAS_DIRECT_DEP', 'CALC_CURRENT_MONTH_END_SHARE_BALANCE', 'NEG_SHARE_BALANCE', 'LOYALTY_DUMMY_MEMBER', 'CALC_PRIOR_MONTH_END_SHARE_BALANCE', 'INT_ACCRUED', 'new_england', 'Diff_OPEN_DATE_CREDIT_SCORE']]\n",
    "\n",
    "# Create design matrix\n",
    "X = dmatrix(\"BALANCE + LIMIT + PRODUCT_NAME + TIMES_DELQ_30_DAYS + TIMES_DELQ_90_DAYS + CREDIT_SCORE + SEASONED_OVER_2YR + BANKPRUPTCY_FLAG + HAS_DIRECT_DEP + CALC_CURRENT_MONTH_END_SHARE_BALANCE + NEG_SHARE_BALANCE + LOYALTY_DUMMY_MEMBER + CALC_PRIOR_MONTH_END_SHARE_BALANCE + INT_ACCRUED + new_england + Diff_OPEN_DATE_CREDIT_SCORE - 1\", data=df3, return_type='dataframe')\n",
    "\n",
    "# Create binary target variable\n",
    "y = (df['Recovery'] > 0).astype(int)\n",
    "\n",
    "# Resample data\n",
    "positive_indices = np.where(y == 1)[0]\n",
    "negative_indices = np.where(y == 0)[0]\n",
    "random_negative_indices = np.random.choice(negative_indices, size=3305, replace=True)\n",
    "resampled_indices = np.concatenate([positive_indices, random_negative_indices])\n",
    "\n",
    "# Use resampled indices to get the corresponding labels and features\n",
    "X_resampled = X.iloc[resampled_indices]\n",
    "y_resampled = y.iloc[resampled_indices]\n",
    "\n",
    "# Fit logistic regression model on the resampled data\n",
    "glm_resampled = sm.GLM(y_resampled, X_resampled, family=sm.families.Binomial())\n",
    "results_resampled = glm_resampled.fit()\n",
    "\n",
    "# Summarize results for resampled data\n",
    "print(results_resampled.summary())\n",
    "\n",
    "# Predict probabilities using the fitted model on the original data\n",
    "probs = results_resampled.predict(X_resampled)\n",
    "\n",
    "# Set a threshold (e.g., 0.5) to classify probabilities into binary predictions\n",
    "threshold = 0.2\n",
    "binary_predictions = (probs > threshold).astype(int)\n",
    "\n",
    "# Create a confusion matrix\n",
    "conf_matrix = confusion_matrix(y_resampled, binary_predictions)\n",
    "\n",
    "# Print the confusion matrix\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_resampled, binary_predictions)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Editing the columns in Train and test sets after finding out VIF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa1e6b80",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_keep = ['BALANCE', 'LIMIT', 'PRODUCT_NAME', 'TIMES_DELQ_30_DAYS',\n",
    "                   'TIMES_DELQ_90_DAYS', 'CREDIT_SCORE', 'SEASONED_OVER_2YR',\n",
    "                   'BANKPRUPTCY_FLAG', 'HAS_DIRECT_DEP', 'CALC_CURRENT_MONTH_END_SHARE_BALANCE',\n",
    "                   'NEG_SHARE_BALANCE', 'LOYALTY_DUMMY_MEMBER', 'CALC_PRIOR_MONTH_END_SHARE_BALANCE',\n",
    "                   'INT_ACCRUED', 'new_england', 'Diff_OPEN_DATE_CREDIT_SCORE', 'Recovery']\n",
    "\n",
    "# Keep only the specified columns in train_set\n",
    "train_set = train_set.loc[:, columns_to_keep]\n",
    "\n",
    "# Keep only the specified columns in test_set_1\n",
    "test_set_1 = test_set_1.loc[:, columns_to_keep]\n",
    "\n",
    "# Keep only the specified columns in test_set_2\n",
    "test_set_2 = test_set_2.loc[:, columns_to_keep]\n",
    "\n",
    "# Keep only the specified columns in test_set_3\n",
    "test_set_3 = test_set_3.loc[:, columns_to_keep]\n",
    "\n",
    "# Keep only the specified columns in test_set_4\n",
    "test_set_4 = test_set_4.loc[:, columns_to_keep]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18e7e4e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "from patsy import dmatrix\n",
    "\n",
    "# Target Column\n",
    "target_column = 'Recovery'\n",
    "\n",
    "# Variables\n",
    "selected_columns = ['BALANCE', 'LIMIT', 'PRODUCT_NAME', 'TIMES_DELQ_30_DAYS', 'TIMES_DELQ_90_DAYS',\n",
    "                     'CREDIT_SCORE', 'SEASONED_OVER_2YR', 'BANKPRUPTCY_FLAG', 'HAS_DIRECT_DEP',\n",
    "                     'CALC_CURRENT_MONTH_END_SHARE_BALANCE', 'NEG_SHARE_BALANCE',\n",
    "                     'LOYALTY_DUMMY_MEMBER', 'CALC_PRIOR_MONTH_END_SHARE_BALANCE',\n",
    "                     'INT_ACCRUED', 'new_england', 'Diff_OPEN_DATE_CREDIT_SCORE']\n",
    "\n",
    "# Create design matrix for the selected columns in the training set\n",
    "X_train = dmatrix(\"BALANCE + LIMIT + PRODUCT_NAME + TIMES_DELQ_30_DAYS + TIMES_DELQ_90_DAYS + CREDIT_SCORE + SEASONED_OVER_2YR + BANKPRUPTCY_FLAG + HAS_DIRECT_DEP + CALC_CURRENT_MONTH_END_SHARE_BALANCE + NEG_SHARE_BALANCE + LOYALTY_DUMMY_MEMBER + CALC_PRIOR_MONTH_END_SHARE_BALANCE + INT_ACCRUED + new_england + Diff_OPEN_DATE_CREDIT_SCORE - 1\", data=train_set, return_type='dataframe')\n",
    "\n",
    "# Reset index of X_train to ensure proper alignment\n",
    "X_train = X_train.reset_index(drop=True)\n",
    "\n",
    "# Create binary target variable for the training set\n",
    "y_train = (train_set['Recovery'] > 0).astype(int)\n",
    "\n",
    "# Set the index of y_train to match the index of X_train\n",
    "y_train.index = X_train.index\n",
    "\n",
    "# Fit logistic regression model on the training data\n",
    "glm_train = sm.GLM(y_train, X_train, family=sm.families.Binomial())\n",
    "results_train = glm_train.fit()\n",
    "\n",
    "# Summarize results for the training data\n",
    "print(results_train.summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4413bba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix \n",
    "confusion_matrix(y_train, train_set['BANKPRUPTCY_FLAG'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07468f56",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "\n",
    "# Predict probabilities using the fitted model on the original data\n",
    "probs_train = results_train.predict(exog=X_train)\n",
    "\n",
    "# Set a threshold (e.g., 0.2) to classify probabilities into binary predictions for the training set\n",
    "threshold = 0.2\n",
    "binary_predictions_train = (probs_train > threshold).astype(int)\n",
    "\n",
    "y_train = (train_set['Recovery'] > 0).astype(int)\n",
    "\n",
    "# Reset indices to ensure they match\n",
    "y_train.index = binary_predictions_train.index\n",
    "\n",
    "# Create a confusion matrix for the training set\n",
    "conf_matrix_train = confusion_matrix(y_train, binary_predictions_train)\n",
    "\n",
    "# Print the confusion matrix for the training set\n",
    "print(\"Confusion Matrix (Training Set):\")\n",
    "print(conf_matrix_train)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9af43ef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_train, binary_predictions_train)\n",
    "\n",
    "# Calculate sensitivity\n",
    "sensitivity = conf_matrix_train[1, 1] / (conf_matrix_train[1, 1] + conf_matrix_train[1, 0])\n",
    "\n",
    "# Calculate specificity\n",
    "specificity = conf_matrix_train[0, 0] / (conf_matrix_train[0, 0] + conf_matrix_train[0, 1])\n",
    "\n",
    "# Calculate prevalence\n",
    "prevalence = (conf_matrix_train[1, 1] + conf_matrix_train[1, 0]) / np.sum(conf_matrix_train)\n",
    "\n",
    "# Print\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Sensitivity (True Positive Rate):\", sensitivity)\n",
    "print(\"Specificity (True Negative Rate):\", specificity)\n",
    "print(\"Prevalence:\", prevalence)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91feba30",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "selected_features = ['BALANCE', 'LIMIT', 'PRODUCT_NAME', 'TIMES_DELQ_30_DAYS', 'TIMES_DELQ_90_DAYS',\n",
    "                     'CREDIT_SCORE', 'SEASONED_OVER_2YR', 'BANKPRUPTCY_FLAG', 'HAS_DIRECT_DEP',\n",
    "                     'CALC_CURRENT_MONTH_END_SHARE_BALANCE', 'NEG_SHARE_BALANCE',\n",
    "                     'LOYALTY_DUMMY_MEMBER', 'CALC_PRIOR_MONTH_END_SHARE_BALANCE',\n",
    "                     'INT_ACCRUED', 'new_england', 'Diff_OPEN_DATE_CREDIT_SCORE']\n",
    "\n",
    "# Create design matrix for the selected features in the test set\n",
    "X_test = dmatrix(\"BALANCE + LIMIT + PRODUCT_NAME + TIMES_DELQ_30_DAYS + TIMES_DELQ_90_DAYS + CREDIT_SCORE + SEASONED_OVER_2YR + BANKPRUPTCY_FLAG + HAS_DIRECT_DEP + CALC_CURRENT_MONTH_END_SHARE_BALANCE + NEG_SHARE_BALANCE + LOYALTY_DUMMY_MEMBER + CALC_PRIOR_MONTH_END_SHARE_BALANCE + INT_ACCRUED + new_england + Diff_OPEN_DATE_CREDIT_SCORE - 1\", data=test_set_1, return_type='dataframe')\n",
    "\n",
    "# Reset index of X_test to ensure proper alignment\n",
    "X_test = X_test.reset_index(drop=True)\n",
    "\n",
    "# Predict probabilities using the fitted model on the test set\n",
    "probs_test = results_train.predict(exog=X_test)\n",
    "\n",
    "# Set the same threshold used for training to classify probabilities into binary predictions for the test set\n",
    "binary_predictions_test = (probs_test > threshold).astype(int)\n",
    "\n",
    "y_test = (test_set_1['Recovery'] > 0).astype(int)\n",
    "\n",
    "# Reset indices to ensure they match\n",
    "y_test.index = binary_predictions_test.index\n",
    "\n",
    "# Create a confusion matrix for the test set\n",
    "conf_matrix_test = confusion_matrix(y_test, binary_predictions_test)\n",
    "\n",
    "# Print the confusion matrix for the test set\n",
    "print(\"Confusion Matrix (Test Set 1):\")\n",
    "print(conf_matrix_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37813b86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate sensitivity, specificity, prevalence, and accuracy for the test set\n",
    "\n",
    "# Accuracy\n",
    "accuracy_test = accuracy_score(y_test, binary_predictions_test)\n",
    "\n",
    "# Sensitivity (True Positive Rate or Recall)\n",
    "sensitivity_test = conf_matrix_test[1, 1] / (conf_matrix_test[1, 1] + conf_matrix_test[1, 0])\n",
    "\n",
    "# Specificity (True Negative Rate)\n",
    "specificity_test = conf_matrix_test[0, 0] / (conf_matrix_test[0, 0] + conf_matrix_test[0, 1])\n",
    "\n",
    "# Prevalence\n",
    "prevalence_test = (conf_matrix_test[1, 1] + conf_matrix_test[1, 0]) / np.sum(conf_matrix_test)\n",
    "\n",
    "\n",
    "# Print the results for the test set\n",
    "print(\"Accuracy:\", accuracy_test)\n",
    "print(\"Sensitivity (True Positive Rate):\", sensitivity_test)\n",
    "print(\"Specificity (True Negative Rate):\", specificity_test)\n",
    "print(\"Prevalence:\", prevalence_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69f27869",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "selected_features = ['BALANCE', 'LIMIT', 'PRODUCT_NAME', 'TIMES_DELQ_30_DAYS', 'TIMES_DELQ_90_DAYS',\n",
    "                     'CREDIT_SCORE', 'SEASONED_OVER_2YR', 'BANKPRUPTCY_FLAG', 'HAS_DIRECT_DEP',\n",
    "                     'CALC_CURRENT_MONTH_END_SHARE_BALANCE', 'NEG_SHARE_BALANCE',\n",
    "                     'LOYALTY_DUMMY_MEMBER', 'CALC_PRIOR_MONTH_END_SHARE_BALANCE',\n",
    "                     'INT_ACCRUED', 'new_england', 'Diff_OPEN_DATE_CREDIT_SCORE']\n",
    "\n",
    "# Create design matrix for the selected features in the test set\n",
    "X_test_2 = dmatrix(\"BALANCE + LIMIT + PRODUCT_NAME + TIMES_DELQ_30_DAYS + TIMES_DELQ_90_DAYS + CREDIT_SCORE + SEASONED_OVER_2YR + BANKPRUPTCY_FLAG + HAS_DIRECT_DEP + CALC_CURRENT_MONTH_END_SHARE_BALANCE + NEG_SHARE_BALANCE + LOYALTY_DUMMY_MEMBER + CALC_PRIOR_MONTH_END_SHARE_BALANCE + INT_ACCRUED + new_england + Diff_OPEN_DATE_CREDIT_SCORE - 1\", data=test_set_2, return_type='dataframe')\n",
    "\n",
    "# Reset index of X_test_2 to ensure proper alignment\n",
    "X_test_2 = X_test_2.reset_index(drop=True)\n",
    "\n",
    "# Predict probabilities using the fitted model on the test set\n",
    "probs_test_2 = results_train.predict(exog=X_test_2)\n",
    "\n",
    "# Set the same threshold used for training to classify probabilities into binary predictions for the test set\n",
    "binary_predictions_test_2 = (probs_test_2 > threshold).astype(int)\n",
    "\n",
    "\n",
    "y_test_2 = (test_set_2['Recovery'] > 0).astype(int)\n",
    "\n",
    "# Reset indices to ensure they match\n",
    "y_test_2.index = binary_predictions_test_2.index\n",
    "\n",
    "# Create a confusion matrix for the test set\n",
    "conf_matrix_test_2 = confusion_matrix(y_test_2, binary_predictions_test_2)\n",
    "\n",
    "# Print the confusion matrix for the test set\n",
    "print(\"Confusion Matrix (Test Set 2):\")\n",
    "print(conf_matrix_test_2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee728260",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate sensitivity, specificity, prevalence, and accuracy for the second test set\n",
    "\n",
    "# Accuracy\n",
    "accuracy_test_2 = accuracy_score(y_test_2, binary_predictions_test_2)\n",
    "\n",
    "# Sensitivity (True Positive Rate or Recall)\n",
    "sensitivity_test_2 = conf_matrix_test_2[1, 1] / (conf_matrix_test_2[1, 1] + conf_matrix_test_2[1, 0])\n",
    "\n",
    "# Specificity (True Negative Rate)\n",
    "specificity_test_2 = conf_matrix_test_2[0, 0] / (conf_matrix_test_2[0, 0] + conf_matrix_test_2[0, 1])\n",
    "\n",
    "# Prevalence\n",
    "prevalence_test_2 = (conf_matrix_test_2[1, 1] + conf_matrix_test_2[1, 0]) / np.sum(conf_matrix_test_2)\n",
    "\n",
    "# Print the results for the second test set\n",
    "print(\"Accuracy:\", accuracy_test_2)\n",
    "print(\"Sensitivity (True Positive Rate):\", sensitivity_test_2)\n",
    "print(\"Specificity (True Negative Rate):\", specificity_test_2)\n",
    "print(\"Prevalence:\", prevalence_test_2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0acf84b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "selected_features = ['BALANCE', 'LIMIT', 'PRODUCT_NAME', 'TIMES_DELQ_30_DAYS', 'TIMES_DELQ_90_DAYS',\n",
    "                     'CREDIT_SCORE', 'SEASONED_OVER_2YR', 'BANKPRUPTCY_FLAG', 'HAS_DIRECT_DEP',\n",
    "                     'CALC_CURRENT_MONTH_END_SHARE_BALANCE', 'NEG_SHARE_BALANCE',\n",
    "                     'LOYALTY_DUMMY_MEMBER', 'CALC_PRIOR_MONTH_END_SHARE_BALANCE',\n",
    "                     'INT_ACCRUED', 'new_england', 'Diff_OPEN_DATE_CREDIT_SCORE']\n",
    "\n",
    "# Create design matrix for the selected features in the test set\n",
    "X_test_3 = dmatrix(\"BALANCE + LIMIT + PRODUCT_NAME + TIMES_DELQ_30_DAYS + TIMES_DELQ_90_DAYS + CREDIT_SCORE + SEASONED_OVER_2YR + BANKPRUPTCY_FLAG + HAS_DIRECT_DEP + CALC_CURRENT_MONTH_END_SHARE_BALANCE + NEG_SHARE_BALANCE + LOYALTY_DUMMY_MEMBER + CALC_PRIOR_MONTH_END_SHARE_BALANCE + INT_ACCRUED + new_england + Diff_OPEN_DATE_CREDIT_SCORE - 1\", data=test_set_3, return_type='dataframe')\n",
    "\n",
    "# Reset index of X_test_3 to ensure proper alignment\n",
    "X_test_3 = X_test_3.reset_index(drop=True)\n",
    "\n",
    "# Predict probabilities using the fitted model on the test set\n",
    "probs_test_3 = results_train.predict(exog=X_test_3)\n",
    "\n",
    "# Set the same threshold used for training to classify probabilities into binary predictions for the test set\n",
    "binary_predictions_test_3 = (probs_test_3 > threshold).astype(int)\n",
    "\n",
    "\n",
    "y_test_3 = (test_set_3['Recovery'] > 0).astype(int)\n",
    "\n",
    "# Reset indices to ensure they match\n",
    "y_test_3.index = binary_predictions_test_3.index\n",
    "\n",
    "# Create a confusion matrix for the test set\n",
    "conf_matrix_test_3 = confusion_matrix(y_test_3, binary_predictions_test_3)\n",
    "\n",
    "# Print the confusion matrix for the test set\n",
    "print(\"Confusion Matrix (Test Set 3):\")\n",
    "print(conf_matrix_test_3)\n",
    "\n",
    "# Calculate accuracy for the test set\n",
    "accuracy_test_3 = accuracy_score(y_test_3, binary_predictions_test_3)\n",
    "print(\"Accuracy (Test Set 3):\", accuracy_test_3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6234607e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate sensitivity, specificity, prevalence, and accuracy for the second test set\n",
    "\n",
    "# Accuracy\n",
    "accuracy_test_3 = accuracy_score(y_test_3, binary_predictions_test_3)\n",
    "\n",
    "# Sensitivity (True Positive Rate or Recall)\n",
    "sensitivity_test_3 = conf_matrix_test_3[1, 1] / (conf_matrix_test_3[1, 1] + conf_matrix_test_3[1, 0])\n",
    "\n",
    "# Specificity (True Negative Rate)\n",
    "specificity_test_3 = conf_matrix_test_3[0, 0] / (conf_matrix_test_3[0, 0] + conf_matrix_test_3[0, 1])\n",
    "\n",
    "# Prevalence\n",
    "prevalence_test_3 = (conf_matrix_test_3[1, 1] + conf_matrix_test_3[1, 0]) / np.sum(conf_matrix_test_3)\n",
    "\n",
    "# Print the results for the second test set\n",
    "print(\"Accuracy:\", accuracy_test_3)\n",
    "print(\"Sensitivity (True Positive Rate):\", sensitivity_test_3)\n",
    "print(\"Specificity (True Negative Rate):\", specificity_test_3)\n",
    "print(\"Prevalence:\", prevalence_test_3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb7885ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "selected_features = ['BALANCE', 'LIMIT', 'PRODUCT_NAME', 'TIMES_DELQ_30_DAYS', 'TIMES_DELQ_90_DAYS',\n",
    "                     'CREDIT_SCORE', 'SEASONED_OVER_2YR', 'BANKPRUPTCY_FLAG', 'HAS_DIRECT_DEP',\n",
    "                     'CALC_CURRENT_MONTH_END_SHARE_BALANCE', 'NEG_SHARE_BALANCE',\n",
    "                     'LOYALTY_DUMMY_MEMBER', 'CALC_PRIOR_MONTH_END_SHARE_BALANCE',\n",
    "                     'INT_ACCRUED', 'new_england', 'Diff_OPEN_DATE_CREDIT_SCORE']\n",
    "\n",
    "# Create design matrix for the selected features in the test set\n",
    "X_test_4 = dmatrix(\"BALANCE + LIMIT + PRODUCT_NAME + TIMES_DELQ_30_DAYS + TIMES_DELQ_90_DAYS + CREDIT_SCORE + SEASONED_OVER_2YR + BANKPRUPTCY_FLAG + HAS_DIRECT_DEP + CALC_CURRENT_MONTH_END_SHARE_BALANCE + NEG_SHARE_BALANCE + LOYALTY_DUMMY_MEMBER + CALC_PRIOR_MONTH_END_SHARE_BALANCE + INT_ACCRUED + new_england + Diff_OPEN_DATE_CREDIT_SCORE - 1\", data=test_set_4, return_type='dataframe')\n",
    "\n",
    "# Reset index of X_test_4 to ensure proper alignment\n",
    "X_test_4 = X_test_4.reset_index(drop=True)\n",
    "\n",
    "# Predict probabilities using the fitted model on the test set\n",
    "probs_test_4 = results_train.predict(exog=X_test_4)\n",
    "\n",
    "# Set the same threshold used for training to classify probabilities into binary predictions for the test set\n",
    "binary_predictions_test_4 = (probs_test_4 > threshold).astype(int)\n",
    "\n",
    "y_test_4 = (test_set_4['Recovery'] > 0).astype(int)\n",
    "\n",
    "# Reset indices to ensure they match\n",
    "y_test_4.index = binary_predictions_test_4.index\n",
    "\n",
    "# Create a confusion matrix for the test set\n",
    "conf_matrix_test_4 = confusion_matrix(y_test_4, binary_predictions_test_4)\n",
    "\n",
    "# Print the confusion matrix for the test set\n",
    "print(\"Confusion Matrix (Test Set 4):\")\n",
    "print(conf_matrix_test_4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f7ecab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate sensitivity, specificity, prevalence, and accuracy for the second test set\n",
    "\n",
    "# Accuracy\n",
    "accuracy_test_4 = accuracy_score(y_test_4, binary_predictions_test_4)\n",
    "\n",
    "# Sensitivity (True Positive Rate or Recall)\n",
    "sensitivity_test_4 = conf_matrix_test_4[1, 1] / (conf_matrix_test_4[1, 1] + conf_matrix_test_4[1, 0])\n",
    "\n",
    "# Specificity (True Negative Rate)\n",
    "specificity_test_4 = conf_matrix_test_4[0, 0] / (conf_matrix_test_4[0, 0] + conf_matrix_test_4[0, 1])\n",
    "\n",
    "# Prevalence\n",
    "prevalence_test_4 = (conf_matrix_test_4[1, 1] + conf_matrix_test_4[1, 0]) / np.sum(conf_matrix_test_4)\n",
    "\n",
    "# Print the results for the second test set\n",
    "print(\"Accuracy:\", accuracy_test_4)\n",
    "print(\"Sensitivity (True Positive Rate):\", sensitivity_test_4)\n",
    "print(\"Specificity (True Negative Rate):\", specificity_test_4)\n",
    "print(\"Prevalence:\", prevalence_test_4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"yellow\">MODEL 2: LINEAR REGRESSION</font>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b68aa3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using only the observation where Recovery is > $0. \n",
    "\n",
    "df1 = df.copy()\n",
    "df1 = df[df['Recovery'] > 0]\n",
    "print(df1)\n",
    "\n",
    "# List of columns to keep\n",
    "columns_to_keep = [\n",
    "    'LIMIT',\n",
    "    'PRODUCT_NAME',\n",
    "    'TIMES_DELQ_30_DAYS',\n",
    "    'TIMES_DELQ_90_DAYS',\n",
    "    'CREDIT_SCORE',\n",
    "    'SEASONED_OVER_2YR',\n",
    "    'BANKPRUPTCY_FLAG',\n",
    "    'HAS_DIRECT_DEP',\n",
    "    'CALC_CURRENT_MONTH_END_SHARE_BALANCE',\n",
    "    'NEG_SHARE_BALANCE',\n",
    "    'LOYALTY_DUMMY_MEMBER',\n",
    "    'CALC_PRIOR_MONTH_END_SHARE_BALANCE',\n",
    "    'INT_ACCRUED',\n",
    "    'new_england',\n",
    "    'Diff_OPEN_DATE_CREDIT_SCORE',\n",
    "    'BALANCE',\n",
    "    'Recovery'\n",
    "]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "685ba2ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Visualizations\n",
    "fp = r\"New_updated.xlsx\"\n",
    "df1.to_excel(fp, sheet_name='Edit 3')\n",
    "\n",
    "\n",
    "print(\"Modified data saved to\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cfa2c12",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create the new DataFrame with selected columns\n",
    "df2 = df1[columns_to_keep]\n",
    "print(df2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1a214d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Result without training and testing\n",
    "X = df2.drop('Recovery', axis=1)\n",
    "y = df1['Recovery']\n",
    "X = sm.add_constant(X)  # Add a constant term (intercept) to the independent variables\n",
    "print(X.shape)\n",
    "print(y.shape)\n",
    "\n",
    "model = sm.OLS(y, X)\n",
    "results = model.fit()\n",
    "print(results.summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19a223ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the predicted values\n",
    "predicted_Recovery = results.fittedvalues\n",
    "\n",
    "# Add the predicted_Recovery column to df2\n",
    "df2['predicted_Recovery'] = predicted_Recovery\n",
    "\n",
    "\n",
    "# Calculate the residuals\n",
    "residuals = y - predicted_Recovery\n",
    "\n",
    "# Create a scatter plot of residuals vs. the target variable\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.scatter(y, residuals, alpha=0.2)\n",
    "plt.xlabel('Target Variable (Recovery)')\n",
    "plt.ylabel('Residuals')\n",
    "plt.title('Residuals vs. Target Variable')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd4029af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scatter plot of residuals vs. the target variable\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.scatter(y, df2['TIMES_DELQ_30_DAYS'], alpha=0.2)\n",
    "plt.xlabel('Recovery($)')\n",
    "plt.ylabel('TIMES_DELQ_30_DAYS')\n",
    "plt.title('TIMES_DELQ_30_DAYS vs. Target Variable')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bab7ab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scatter plot of residuals vs. the target variable\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.scatter(y, df2['TIMES_DELQ_90_DAYS'], alpha=0.2)\n",
    "plt.xlabel('Recovery($)')\n",
    "plt.ylabel('TIMES_DELQ_90_DAYS')\n",
    "plt.title('TIMES_DELQ_90_DAYS vs. Target Variable')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9c522b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new columns TrueLGD & PredictedLGD\n",
    "df2['TrueLGD'] = 1 - (df2['Recovery'] / df2['BALANCE'])\n",
    "df2['PredictedLGD'] = 1 - (df2['predicted_Recovery'] / df2['BALANCE'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f4c3342",
   "metadata": {},
   "source": [
    "Add the columns to the train & test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18ec071a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df2[['TrueLGD', 'predicted_Recovery', 'PredictedLGD']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c68cd2f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d769058e",
   "metadata": {},
   "outputs": [],
   "source": [
    "med = df[df['Recovery'] > 0]\n",
    "df2['MONTH_END_DATE'] = med['MONTH_END_DATE']\n",
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dd9c18d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into train and test sets\n",
    "train_set_copy = df2.loc[(df2['MONTH_END_DATE'] >= '2013-01-01') & (df2['MONTH_END_DATE'] <= '2017-12-31'), :]\n",
    "\n",
    "test_set_1_copy = df2.loc[(df2['MONTH_END_DATE'] >= '2018-01-01') & (df2['MONTH_END_DATE'] <= '2020-03-31'), :]\n",
    "test_set_2_copy = df2.loc[(df2['MONTH_END_DATE'] >= '2020-04-01') & (df2['MONTH_END_DATE'] <= '2020-12-31'), :]\n",
    "test_set_3_copy = df2.loc[(df2['MONTH_END_DATE'] >= '2021-01-01') & (df2['MONTH_END_DATE'] <= '2021-12-31'), :]\n",
    "test_set_4_copy = df2.loc[(df2['MONTH_END_DATE'] >= '2022-01-01') & (df2['MONTH_END_DATE'] <= '2022-12-31'), :]\n",
    "\n",
    "# Print \n",
    "print(\"Train Set Copy Shape:\", train_set_copy.shape)\n",
    "print(\"Test Set 1 copy Shape:\", test_set_1_copy.shape)\n",
    "print(\"Test Set 2 copy Shape:\", test_set_2_copy.shape)\n",
    "print(\"Test Set 3 copy Shape:\", test_set_3_copy.shape)\n",
    "print(\"Test Set 4 copy Shape:\", test_set_4_copy.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ff0f005",
   "metadata": {},
   "source": [
    "Dropping MED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc6662a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop 'MONTH_END_DATE' column from train_set\n",
    "train_set_copy = train_set_copy.drop('MONTH_END_DATE', axis=1)\n",
    "\n",
    "# Drop 'MONTH_END_DATE' column from test_set_1\n",
    "test_set_1_copy = test_set_1_copy.drop('MONTH_END_DATE', axis=1)\n",
    "test_set_1_copy_X = test_set_1_copy.drop(['Recovery','predicted_Recovery', 'TrueLGD', 'PredictedLGD'], axis=1)\n",
    "test_set_1_copy_X = sm.add_constant(test_set_1_copy_X)\n",
    "# Drop 'MONTH_END_DATE' column from test_set_2\n",
    "test_set_2_copy = test_set_2_copy.drop('MONTH_END_DATE', axis=1)\n",
    "test_set_2_copy_X = test_set_2_copy.drop(['Recovery','predicted_Recovery', 'TrueLGD', 'PredictedLGD'], axis=1)\n",
    "test_set_2_copy_X = sm.add_constant(test_set_2_copy_X)\n",
    "# Drop 'MONTH_END_DATE' column from test_set_3\n",
    "test_set_3_copy = test_set_3_copy.drop('MONTH_END_DATE', axis=1)\n",
    "test_set_3_copy_X = test_set_3_copy.drop(['Recovery','predicted_Recovery', 'TrueLGD', 'PredictedLGD'], axis=1)\n",
    "test_set_3_copy_X = sm.add_constant(test_set_3_copy_X)\n",
    "# Drop 'MONTH_END_DATE' column from test_set_4\n",
    "test_set_4_copy = test_set_4_copy.drop('MONTH_END_DATE', axis=1)\n",
    "test_set_4_copy_X = test_set_4_copy.drop(['Recovery','predicted_Recovery', 'TrueLGD', 'PredictedLGD'], axis=1)\n",
    "test_set_4_copy_X = sm.add_constant(test_set_4_copy_X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c57f1f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set_1_copy_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3a0990e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X = train_set_copy.drop(['Recovery','predicted_Recovery', 'TrueLGD', 'PredictedLGD'], axis=1)\n",
    "y = train_set_copy['Recovery']\n",
    "y = y.clip(lower=0 )\n",
    "X = sm.add_constant(X)  # Add a constant term (intercept) to the independent variables\n",
    "print(X.shape)\n",
    "print(y.shape)\n",
    "\n",
    "model = sm.OLS(y, X)\n",
    "results = model.fit()\n",
    "print(results.summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da14697b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set_predict = results.predict(X)\n",
    "\n",
    "column1 = 1 - (train_set_copy['Recovery'] / train_set_copy['BALANCE'])\n",
    "column2 = 1 - (train_set_predict/ train_set_copy['BALANCE'])\n",
    "\n",
    "# Clip values to be between 0 and 1\n",
    "column1 = column1.clip(0, 1)\n",
    "column2 = column2.clip(0, 1)\n",
    "\n",
    "# Mean Squared Error (MSE)\n",
    "mse = np.mean((column1 - column2)**2)\n",
    "\n",
    "# Print the MSE result\n",
    "print(\"Mean Squared Error (MSE):\", mse)\n",
    "\n",
    "# Calculate Sum of Squared Residuals (SSR)\n",
    "ssr = ((column1 - column2)**2).sum()\n",
    "\n",
    "# Print the SSR result\n",
    "print(\"Sum of Squared Residuals (SSR):\", ssr)\n",
    "\n",
    "# Calculate Total Sum of Squares (SST)\n",
    "sst = ((column1 - column1.mean())**2).sum()\n",
    "\n",
    "# Print the SST result\n",
    "print(\"Total Sum of Squares (SST):\", sst)\n",
    "\n",
    "# Calculate R-squared (R²)\n",
    "r_squared = 1 - (ssr / sst)\n",
    "\n",
    "# Print the R-squared result\n",
    "print(\"R-squared (R²):\", r_squared)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2e9f756",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set_1_copy_pred = results.predict(test_set_1_copy_X)\n",
    "\n",
    "column1 = 1 - (test_set_1_copy['Recovery'] / test_set_1_copy['BALANCE'])\n",
    "column2 = 1 - (test_set_1_copy_pred/ test_set_1_copy['BALANCE'])\n",
    "\n",
    "# Clip values to be between 0 and 1\n",
    "column1 = column1.clip(0, 1)\n",
    "column2 = column2.clip(0, 1)\n",
    "\n",
    "## Mean Squared Error (MSE)\n",
    "mse = np.mean((column1 - column2)**2)\n",
    "\n",
    "# Print the MSE result\n",
    "print(\"Mean Squared Error (MSE):\", mse)\n",
    "\n",
    "## Calculate Sum of Squared Residuals (SSR)\n",
    "ssr = ((column1 - column2)**2).sum()\n",
    "\n",
    "# Print the SSR result\n",
    "print(\"Sum of Squared Residuals (SSR):\", ssr)\n",
    "\n",
    "## Calculate Total Sum of Squares (SST)\n",
    "sst = ((column1 - column1.mean())**2).sum()\n",
    "\n",
    "# Print the SST result\n",
    "print(\"Total Sum of Squares (SST):\", sst)\n",
    "\n",
    "## Calculate R-squared (R²)\n",
    "r_squared = 1 - (ssr / sst)\n",
    "\n",
    "# Print the R-squared result\n",
    "print(\"R-squared (R²):\", r_squared)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd4fb478",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set_2_copy_pred = results.predict(test_set_2_copy_X)\n",
    "\n",
    "column1 = 1 - (test_set_2_copy['Recovery'] / test_set_2_copy['BALANCE'])\n",
    "column2 = 1 - (test_set_2_copy_pred/ test_set_2_copy['BALANCE'])\n",
    "\n",
    "# Clip values to be between 0 and 1\n",
    "column1 = column1.clip(0, 1)\n",
    "column2 = column2.clip(0, 1)\n",
    "\n",
    "## Mean Squared Error (MSE)\n",
    "mse = np.mean((column1 - column2)**2)\n",
    "\n",
    "# Print the MSE result\n",
    "print(\"Mean Squared Error (MSE):\", mse)\n",
    "\n",
    "## Calculate Sum of Squared Residuals (SSR)\n",
    "ssr = ((column1 - column2)**2).sum()\n",
    "\n",
    "# Print the SSR result\n",
    "print(\"Sum of Squared Residuals (SSR):\", ssr)\n",
    "\n",
    "## Calculate Total Sum of Squares (SST)\n",
    "sst = ((column1 - column1.mean())**2).sum()\n",
    "\n",
    "# Print the SST result\n",
    "print(\"Total Sum of Squares (SST):\", sst)\n",
    "\n",
    "## Calculate R-squared (R²)\n",
    "r_squared = 1 - (ssr / sst)\n",
    "\n",
    "# Print the R-squared result\n",
    "print(\"R-squared (R²):\", r_squared)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c6be4db",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set_3_copy_pred = results.predict(test_set_3_copy_X)\n",
    "\n",
    "column1 = 1 - (test_set_3_copy['Recovery'] / test_set_3_copy['BALANCE'])\n",
    "column2 = 1 - (test_set_3_copy_pred/ test_set_3_copy['BALANCE'])\n",
    "\n",
    "# Clip values to be between 0 and 1\n",
    "column1 = column1.clip(0, 1)\n",
    "column2 = column2.clip(0, 1)\n",
    "\n",
    "## Mean Squared Error (MSE)\n",
    "mse = np.mean((column1 - column2)**2)\n",
    "\n",
    "# Print the MSE result\n",
    "print(\"Mean Squared Error (MSE):\", mse)\n",
    "\n",
    "## Calculate Sum of Squared Residuals (SSR)\n",
    "ssr = ((column1 - column2)**2).sum()\n",
    "\n",
    "# Print the SSR result\n",
    "print(\"Sum of Squared Residuals (SSR):\", ssr)\n",
    "\n",
    "## Calculate Total Sum of Squares (SST)\n",
    "sst = ((column1 - column1.mean())**2).sum()\n",
    "\n",
    "# Print the SST result\n",
    "print(\"Total Sum of Squares (SST):\", sst)\n",
    "\n",
    "## Calculate R-squared (R²)\n",
    "r_squared = 1 - (ssr / sst)\n",
    "\n",
    "# Print the R-squared result\n",
    "print(\"R-squared (R²):\", r_squared)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed1a782e",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set_4_copy_pred = results.predict(test_set_4_copy_X)\n",
    "\n",
    "column1 = 1 - (test_set_4_copy['Recovery'] / test_set_4_copy['BALANCE'])\n",
    "column2 = 1 - (test_set_4_copy_pred/ test_set_4_copy['BALANCE'])\n",
    "\n",
    "# Clip values to be between 0 and 1\n",
    "column1 = column1.clip(0, 1)\n",
    "column2 = column2.clip(0, 1)\n",
    "\n",
    "## Mean Squared Error (MSE)\n",
    "mse = np.mean((column1 - column2)**2)\n",
    "\n",
    "# Print the MSE result\n",
    "print(\"Mean Squared Error (MSE):\", mse)\n",
    "\n",
    "## Calculate Sum of Squared Residuals (SSR)\n",
    "ssr = ((column1 - column2)**2).sum()\n",
    "\n",
    "# Print the SSR result\n",
    "print(\"Sum of Squared Residuals (SSR):\", ssr)\n",
    "\n",
    "## Calculate Total Sum of Squares (SST)\n",
    "sst = ((column1 - column1.mean())**2).sum()\n",
    "\n",
    "# Print the SST result\n",
    "print(\"Total Sum of Squares (SST):\", sst)\n",
    "\n",
    "## Calculate R-squared (R²)\n",
    "r_squared = 1 - (ssr / sst)\n",
    "\n",
    "# Print the R-squared result\n",
    "print(\"R-squared (R²):\", r_squared)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
